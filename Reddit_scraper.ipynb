{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit API scraper\n",
    "\n",
    "In the final dataset used for this project, we scraped new comments directly from the Reddit API. We wanted to compare data from 2019 with data from 2024, so we used the API to obtain the most recent data.\n",
    "\n",
    "We followed the tutorial found on [Medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for this scraper. We highly recommend it as it was very clear and super helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect with the API, you need a Reddit account and must use your credentials to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salomenkb/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('<CLIENT_ID>', '<SECRET_TOKEN>')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': '<USERNAME>',\n",
    "        'password': '<PASSWORD>'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the fun begins !\n",
    "\n",
    "Once connected, this is where the fun begins. After examining the data we could extract from the API, we navigated to the specific information we needed: the comments, stored under the key \"body.\" As you can imagine, Reddit contains a vast amount of information surrounding each post and comment. The response came as a nested JSON file, requiring us to dig through it to find the relevant data. We successfully located the \"body\" key and extracted the comment, the user, the date/time, and the title of the post it was associated with.\n",
    "\n",
    "When dealing with scraped data, proper cleaning is often necessary, especially with textual data, where encoded characters are common. The clean_comment function helps clean each scraped comment as thoroughly as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_comment(comment):\n",
    "    # Converting the comment to bytes and then decode using utf-8 and unicode_escape\n",
    "    comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n",
    "    \n",
    "    # Decoding HTML entities if any\n",
    "    comment = html.unescape(comment_bytes)\n",
    "    \n",
    "    # Replacing specific sequences with their corresponding characters\n",
    "    replacements = {\n",
    "        '\\u00e2\\u0080\\u0099': \"'\",  # Apostrophe\n",
    "        '\\u00e2\\u0080\\u009c': '\"',  # Left double quotation mark\n",
    "        '\\u00e2\\u0080\\u009d': '\"',  # Right double quotation mark\n",
    "        '\\u00c2': ''                  # Non-breaking space (Ã‚)\n",
    "    }\n",
    "    \n",
    "    for target, replacement in replacements.items():\n",
    "        comment = comment.replace(target, replacement)\n",
    "    \n",
    "    # Replacing newlines with spaces\n",
    "    comment = comment.replace('\\n', ' ')\n",
    "    \n",
    "    # Removing extra spaces\n",
    "    comment = re.sub(r'\\s+', ' ', comment).strip()\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the relevant information \n",
    "\n",
    "The function 'find_comments' helps extracting the relevant information. As mentionned above we needed the comment, the author, the date/time, the title of the post and the subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comments(data, results, subreddit):\n",
    "    # Check if the current 'data' is a dictionary\n",
    "    if isinstance(data, dict):\n",
    "        # Attempt to retrieve specific keys ('body', 'author', etc.) from the dictionary\n",
    "        body = data.get('body')  # The text of the comment\n",
    "        author = data.get('author')  # The username of the comment's author\n",
    "        title = data.get('permalink')  # The permalink (URL) to the comment or post\n",
    "        created_utc = data.get('created_utc')  # The timestamp of when the comment was created\n",
    "        \n",
    "        # If a 'body' is found (i.e., it's not None), store the relevant data in the 'results' list\n",
    "        if body is not None:\n",
    "            results.append({\n",
    "                'body': clean_comment(body),  # Clean the comment text using the 'clean_comment' function\n",
    "                'author': author,  # Store the author's username\n",
    "                'title': title,  # Store the permalink\n",
    "                'created_utc': created_utc,  # Store the creation timestamp\n",
    "                'subreddit': subreddit  # Store the name of the subreddit\n",
    "            })\n",
    "        \n",
    "        # Recursively process each value in the dictionary, allowing for nested dictionaries\n",
    "        for value in data.values():\n",
    "            find_comments(value, results, subreddit)\n",
    "    \n",
    "    # If 'data' is a list, iterate over each item in the list and process it recursively\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            find_comments(item, results, subreddit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Dig\n",
    "\n",
    "Once everything was ready, it was time to dig through all this information and create a JSONL file with all the comments from the subreddits we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2ut0z\n",
      "1e2sz1e\n",
      "1e2mn9z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\*'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2o8ro\n",
      "1e2kkr5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2hdnp\n",
      "1e2yj02\n",
      "1e2reej\n",
      "1e2a3ho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\['\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2ta3o\n",
      "1e2c0s5\n",
      "1e2c33d\n",
      "1e2hz59\n",
      "1e2j8jv\n",
      "1e2tdkv\n",
      "1e2htyl\n",
      "1e2f282\n",
      "1e2d5v6\n",
      "1e2agz5\n",
      "1e252zk\n",
      "1e1u54t\n",
      "1e1puuc\n",
      "1e21gvt\n",
      "1e1rx8u\n",
      "1e1v1k7\n",
      "1dyaenq\n",
      "1e2po0l\n",
      "1e2q7gq\n",
      "1e2retf\n",
      "1e2n1kr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\)'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2ollj\n",
      "1e2qv6p\n",
      "1e2sr68\n",
      "1e2ooef\n",
      "1e2pmel\n",
      "1e2qo17\n",
      "1e2tyyp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\d'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2kqfv\n",
      "1e2depi\n",
      "1e2j6t6\n",
      "1e2akmo\n",
      "1e2jauq\n",
      "1e2x5i0\n",
      "1e2u7oz\n",
      "1e2lbi5\n",
      "1e2z1uf\n",
      "1e2sh9e\n",
      "1e2ssgl\n",
      "1e2xbrq\n",
      "1e2p82f\n",
      "1e2937k\n",
      "1e2fo1h\n",
      "1311aib\n",
      "1dc8ke8\n",
      "1e2yvdb\n",
      "1e2chzu\n",
      "1e2no70\n",
      "1e2jmes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\`'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2itns\n",
      "1e2ojy1\n",
      "1e2iktj\n",
      "1e2axqr\n",
      "1e2uvol\n",
      "1e2n01d\n",
      "1e27yq8\n",
      "1e1yhpe\n",
      "1e2nglt\n",
      "1e2fcyd\n",
      "1e2xwx1\n",
      "1e2v3ni\n",
      "1e2po1b\n",
      "1e2xjyo\n",
      "1e2ttsw\n",
      "1e28fjy\n",
      "1e2zyg7\n",
      "1e2u1xa\n",
      "1e22hxf\n",
      "1e2y23t\n",
      "1e30olj\n",
      "1e2t4u1\n",
      "1e2ljh1\n",
      "1e2r9gd\n",
      "1e2x07b\n",
      "1e2pzer\n",
      "1e2rmrb\n",
      "1e2fkdg\n",
      "1e2r2yn\n",
      "1e2gzpa\n",
      "1e2aixj\n",
      "1e2jgph\n",
      "1e2zxm5\n",
      "1e2rxru\n",
      "1e2lrsc\n",
      "1e2zhbk\n",
      "1e2b412\n",
      "1e2ll2d\n",
      "1e2px62\n",
      "1e2o7z4\n",
      "1e28dz7\n",
      "1e2qy7j\n",
      "1e2wqja\n",
      "1e30nb0\n",
      "1e2egsk\n",
      "1e27mqh\n",
      "1e308mn\n",
      "1e2zzqf\n",
      "1e2wgpt\n",
      "1e2xtcf\n",
      "1e2vgk8\n",
      "1e2z9qg\n",
      "1e2m6mw\n",
      "1e2vv6c\n",
      "1e2zujd\n",
      "1e2m4p3\n",
      "1e302a9\n",
      "1e2c45t\n",
      "1e2ni2a\n",
      "1e307n3\n",
      "1e28398\n",
      "1e29vi9\n",
      "1e2wh8e\n",
      "1e2vajl\n",
      "1e2zqsb\n",
      "1e28foj\n",
      "1e2zezj\n",
      "1e30i5z\n",
      "1e2yx2z\n",
      "1e2l97v\n",
      "1e2dkhg\n",
      "1e2wpa5\n",
      "1e2dv0i\n",
      "1dsk7b2\n",
      "1e2uf1a\n",
      "1e2mcq3\n",
      "1e2yzbc\n",
      "1e2sy42\n",
      "1e2ujz1\n",
      "1e2wbq0\n",
      "1e2jy9g\n",
      "1e2ybiu\n",
      "1e286wh\n",
      "1e2it9r\n",
      "1e2xrpq\n",
      "1e273mu\n",
      "1e2fu43\n",
      "1e2ke2e\n",
      "1e2qv2f\n",
      "1e28b4x\n",
      "1e2x2z4\n",
      "1e2d8xa\n",
      "1e21azr\n",
      "1e2o9zu\n",
      "1e2qpom\n",
      "1e2vri0\n",
      "1e2ypiy\n",
      "1e27huv\n",
      "1e2ogon\n",
      "1e2v1vu\n",
      "1e2u2mq\n",
      "1e2pdmm\n",
      "1e2i25d\n",
      "1e2lmcs\n",
      "1e2pmoo\n",
      "1e286xq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\-'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2n5kb\n",
      "1e29pcv\n",
      "1e2dvp1\n",
      "1e2abc0\n",
      "1e2z5tz\n",
      "1e2vv0j\n",
      "1e2cnia\n",
      "1e2n2x1\n",
      "1e2hmpx\n",
      "1e272jc\n",
      "1e27g7w\n",
      "1e2xy15\n",
      "1e2y854\n",
      "1e28c6d\n",
      "1e21p7f\n",
      "1e2l4cx\n",
      "1e2p3x2\n",
      "1e2glnl\n",
      "1e2vxee\n",
      "1e2kz9r\n",
      "1e2neh5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/qnp4t1ls5z9b262kt7qlc8fm0000gp/T/ipykernel_1301/1529445398.py:6: DeprecationWarning: invalid escape sequence '\\~'\n",
      "  comment_bytes = comment.encode('utf-8', 'backslashreplace').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2hg3z\n",
      "1e2dkbm\n",
      "1e2j9yp\n",
      "1e2vqe8\n",
      "1e2lly8\n",
      "1e21ivx\n",
      "1e2ihmh\n",
      "1e2dh6x\n",
      "1e24646\n",
      "1e1xw74\n",
      "1e2ngny\n",
      "1e22ac2\n",
      "1e2mrn9\n",
      "1e2d3t5\n",
      "1e2ykoj\n",
      "1e22dx8\n",
      "1e1gb7l\n",
      "1e1t7q2\n",
      "1e2gwzi\n",
      "1e1iffj\n",
      "1e1xpai\n",
      "1e1yu0d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "\n",
    "# List of subreddits to scrape data from\n",
    "subreddits = ['news', 'politics', 'relationship_advice', 'worldnews', 'AskReddit',\n",
    "              'AmItheAsshole', 'todayilearned', 'Showerthoughts']\n",
    "\n",
    "# Looping through each subreddit in the list\n",
    "for subreddit in subreddits:\n",
    "\n",
    "    # Sending a GET request to the Reddit API to retrieve data from the current subreddit\n",
    "    response = requests.get(f\"https://oauth.reddit.com/r/{subreddit}\",\n",
    "                            headers=headers)\n",
    "\n",
    "    # Checking if the request was successful (HTTP status code 200 means OK)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response into a JSON object\n",
    "        data = response.json()\n",
    "\n",
    "        # Extracting the list of posts (children) from the JSON response\n",
    "        children = data[\"data\"][\"children\"]\n",
    "\n",
    "        # Looping through each post in the subreddit\n",
    "        for dictionary in children:\n",
    "            # Store the subreddit name (for potential use in data storage or display)\n",
    "            subreddit = subreddit\n",
    "            \n",
    "            # Extracting the post's unique name (ID), removing the first 3 characters (e.g., 't3_')\n",
    "            post_name = dictionary[\"data\"][\"name\"]\n",
    "            post_name = post_name[3:]\n",
    "            \n",
    "            # Printing the post ID (for debugging or tracking purposes)\n",
    "            print(post_name)\n",
    "            \n",
    "            # Sending a GET request to retrieve comments for the current post\n",
    "            comment_response = requests.get(f\"https://oauth.reddit.com/r/{subreddit}/comments/{post_name}\",\n",
    "                                            headers=headers)\n",
    "            \n",
    "            # Parsing the comments response into a JSON object\n",
    "            comment_data = comment_response.json()\n",
    "            \n",
    "            # Initialize an empty list to store results\n",
    "            results = []\n",
    "\n",
    "            # Attempting to find and extract the 'body' of each comment in the JSON data\n",
    "            for element in comment_data:\n",
    "                try:\n",
    "                    # Custom function to recursively find comments and add them to results\n",
    "                    find_comments(comment_data, results, subreddit)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # Handling JSON decoding errors (e.g., malformed JSON in the response)\n",
    "                    print(f\"Error decoding JSON: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Determine the file mode: 'w' to write a new file or 'a' to append to an existing file\n",
    "            file_mode = 'a' if os.path.exists('comment_data.jsonl') else 'w'\n",
    "\n",
    "            # Writing the extracted comments to a JSONL file (one JSON object per line)\n",
    "            for element in results:\n",
    "                with open('comment_data.jsonl', file_mode) as comment_jsonl:\n",
    "                    json.dump(element, comment_jsonl)\n",
    "                    comment_jsonl.write('\\n')\n",
    "        \n",
    "    else:\n",
    "        # If the initial request to the subreddit fails, print the status code for debugging\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSPT14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
